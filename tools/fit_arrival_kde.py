
import numpy as np
import pandas as pd
from pathlib import Path
import json
from sklearn.neighbors import KernelDensity
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import joblib

# Configuration
DATA_ROOT = Path("outputs_teoricos")
NPZ_DIR = Path("data/arrivals_npz")
OUTPUT_FILE = Path("data/arrival_kde_params.json")
SAMPLE_DAYS = 50  # Limit to avoid memory issues

def load_lambda_series(profile):
    npz_path = NPZ_DIR / f"lambda_{profile}.npz"
    if not npz_path.exists():
        return None
    data = np.load(npz_path, allow_pickle=True)
    return data

def get_lambda_at(t, times, lambdas):
    # Simple step function interpolation
    if t >= times[-1]:
        return lambdas[-1]
    idx = np.searchsorted(times, t, side='right') - 1
    return lambdas[idx]

def fit_arrival_kde():
    print("=== FITTING KDE TO RESCALED INTER-ARRIVAL TIMES ===")
    
    # 1. Collect Rescaled Inter-Arrival Times (RIATs)
    riats = []
    
    # We need to process day by day
    # For each day, we need the actual arrival times AND the lambda series for that day type
    
    # Load lambda series cache
    lambda_cache = {}
    
    processed_count = 0
    for day_dir in sorted(list(DATA_ROOT.glob("Week-*-Day-*"))):
        if processed_count >= SAMPLE_DAYS:
            break
            
        time_log = day_dir / "time_log.csv"
        if not time_log.exists():
            continue
            
        try:
            df = pd.read_csv(time_log, usecols=["event_type", "timestamp_s", "profile", "priority", "payment_method", "items"])
            arrivals = df[df["event_type"] == "arrival"].sort_values("timestamp_s")
            
            if arrivals.empty:
                continue
                
            # Determine Day Type (simplified mapping based on file name or just assume Type 1 for now if not found)
            # Actually, let's try to get day type from folder name mapping if possible, or assume Type 1
            # In engine.py, day type is deterministic by day number.
            day_num = int(day_dir.name.split("-")[-1])
            day_type_map = {
                1: "tipo_1", 2: "tipo_1", 3: "tipo_2", 4: "tipo_1", 
                5: "tipo_2", 6: "tipo_2", 7: "tipo_3"
            }
            day_type = day_type_map.get(day_num, "tipo_1")
            
            # Group by profile/priority/payment to match lambda keys
            # Note: The lambda keys in npz are "day_type|priority|payment|bucket"
            # Simplified: we will just use the aggregate lambda for the profile/day_type to rescale
            # Wait, to do this correctly, we need the EXACT lambda that generated this arrival.
            # But the arrivals are generated by a superposition of processes.
            # Actually, the simulator generates ALL arrivals using a single "Time Rescaling" loop per day?
            # Let's check engine.py again.
            
            # In engine.py:
            # combos = [(prof, pr, pm, ...)...]
            # lam_mat = _build_lambda_matrix(combos, JORNADA)
            # lambda_total_per_sec = lam_mat.sum(axis=0)
            
            # So there is ONE global lambda function for the whole store (or at least the whole simulation run).
            # The simulation runs one "env" per day.
            # So we need to reconstruct the TOTAL lambda for that day.
            
            # To do this, we need to know which profiles were active.
            # We can assume all profiles are active.
            
            # Let's build the total lambda for this day_type
            if day_type not in lambda_cache:
                # Load all profiles
                total_lambda = np.zeros(14 * 3600 + 1) # 8am to 10pm = 14 hours
                # We need to sum up all lambdas for this day_type
                
                for npz_file in NPZ_DIR.glob("lambda_*.npz"):
                    data = np.load(npz_file)
                    keys = data["keys"]
                    lambdas = data["lambdas"]
                    bin_left = data["bin_left_s"]
                    
                    for i, key in enumerate(keys):
                        parts = str(key).split("|")
                        if parts[0] == day_type:
                            # Interpolate to seconds
                            # Lambdas in npz are per minute
                            lam_per_sec = lambdas[i] / 60.0
                            
                            # Expand to seconds
                            # bin_left has indices. We assume 60s bins usually.
                            # Simple expansion:
                            for b in range(len(bin_left)):
                                start = int(bin_left[b])
                                end = int(bin_left[b+1]) if b+1 < len(bin_left) else 14*3600
                                val = lam_per_sec[b]
                                total_lambda[start:end] += val
                                
                lambda_cache[day_type] = total_lambda
            
            lam_series = lambda_cache[day_type]
            
            # Now calculate rescaled times
            # t_i are arrival times
            times = arrivals["timestamp_s"].values
            times = times[(times >= 0) & (times < len(lam_series))]
            
            if len(times) < 2:
                continue
                
            # Sort times
            times = np.sort(times)
            
            # Calculate integral of lambda between t_{i-1} and t_i
            # Integral_a^b lambda(u) du
            # Since we have lambda per second (discrete), sum(lambda[t_{i-1}:t_i])
            
            # Optimization: cumsum
            lam_cumsum = np.cumsum(lam_series)
            
            # Tau_i = Lambda(t_i) - Lambda(t_{i-1})
            # where Lambda(t) is cumulative intensity
            
            # Get cumulative values at arrival times
            # We need to handle float times.
            # Linear interpolation of cumsum
            indices = np.searchsorted(np.arange(len(lam_cumsum)), times)
            # This is approximate.
            
            cum_values = lam_cumsum[indices.clip(0, len(lam_cumsum)-1)]
            
            # Differences
            diffs = np.diff(cum_values)
            
            # Filter out very small diffs (simultaneous arrivals)
            diffs = diffs[diffs > 1e-6]
            
            riats.extend(diffs)
            processed_count += 1
            print(f"Processed {day_dir.name}: {len(diffs)} intervals")
            
        except Exception as e:
            print(f"Error processing {day_dir}: {e}")
            continue

    if not riats:
        print("No data found.")
        return

    riats = np.array(riats)
    print(f"\nTotal Intervals: {len(riats)}")
    mean_riat = np.mean(riats)
    print(f"Mean: {mean_riat:.4f} (Expected ~1.0 for Poisson)")
    print(f"Var:  {np.var(riats):.4f} (Expected ~1.0 for Poisson)")
    
    if abs(mean_riat - 1.0) > 0.5:
        print("\n[WARNING] The mean of Rescaled Inter-Arrival Times is far from 1.0!")
        print("This suggests a mismatch between the theoretical lambda and the observed arrivals.")
        print("Check if 'total_lambda' is correctly summing ALL profiles.")
        print("If the mean is very small (e.g. < 0.1), the simulator will generate WAY too many customers.")
    
    # 2. Fit KDE
    print("\nFitting KDE...")
    
    # Grid search for bandwidth and kernel
    bandwidths = np.logspace(-2, 0.5, 20)
    # Sklearn only supports sampling from gaussian and tophat kernels
    kernels = ['gaussian', 'tophat']
    grid = GridSearchCV(
        KernelDensity(),
        {'bandwidth': bandwidths, 'kernel': kernels},
        cv=5,
        n_jobs=-1  # Use all available cores
    )
    
    # Downsample for fitting if too large
    sample_data = riats
    if len(riats) > 10000:
        sample_data = np.random.choice(riats, 10000, replace=False)
        
    grid.fit(sample_data.reshape(-1, 1))
    
    best_kde = grid.best_estimator_
    print(f"Best Kernel: {best_kde.kernel}")
    print(f"Best Bandwidth: {best_kde.bandwidth}")
    print(f"Best Score: {grid.best_score_}")
    
    # 3. Save Parameters
    # We need to save the kernel type and bandwidth.
    # Also, for the simulator to sample efficiently, we might want to save a pre-computed CDF or just the parameters.
    # Scikit-learn KDE sample() is efficient enough?
    # Engine uses sklearn KDE if available.
    
    # Let's save the parameters to a JSON
    params = {
        "kernel": str(best_kde.kernel),
        "bandwidth": float(best_kde.bandwidth),
        "metric": "euclidean",
        "data_mean": float(np.mean(riats)),
        "data_std": float(np.std(riats))
    }
    
    # Also save a sample of the data (support) to allow reconstruction/sampling
    # We can save the quantiles or a resampled set
    # Saving 1000 points that represent the distribution
    support = best_kde.sample(1000, random_state=42).flatten()
    # Ensure positive
    support = support[support > 0]
    
    # Actually, let's just save the parameters and let the engine use them.
    # But wait, to sample from KDE in engine, we need the "support" data points if we use the custom implementation
    # or we can use sklearn if available.
    # The engine has `_build_kde_model` which takes support and probs.
    # This suggests the engine expects a discrete support + weights (like a histogram or particles).
    # Sklearn KDE stores the training data in `tree_`.
    
    # Let's save the training data (downsampled) as "support" and uniform probs.
    save_data = sample_data
    if len(save_data) > 2000:
        save_data = np.random.choice(save_data, 2000, replace=False)
        
    params["support"] = save_data.tolist()
    params["probs"] = [1.0/len(save_data)] * len(save_data) # Uniform weights
    
    with open(OUTPUT_FILE, "w") as f:
        json.dump(params, f, indent=2)
        
    print(f"Saved KDE parameters to {OUTPUT_FILE}")
    
    # Plot
    plt.figure(figsize=(10, 6))
    plt.hist(riats, bins=100, density=True, alpha=0.5, label="Rescaled IATs")
    x_plot = np.linspace(0, np.percentile(riats, 99), 1000)[:, np.newaxis]
    log_dens = best_kde.score_samples(x_plot)
    plt.plot(x_plot, np.exp(log_dens), label=f"KDE ({best_kde.kernel}, bw={best_kde.bandwidth:.2f})")
    
    # Compare with Exponential(1)
    x_exp = np.linspace(0, np.percentile(riats, 99), 1000)
    plt.plot(x_exp, np.exp(-x_exp), 'r--', label="Exponential(1) (Poisson)")
    
    # Compare with Lognormal(mu=-sigma^2/2, sigma=1.3)
    sigma = 1.3
    mu = -(sigma**2)/2
    pdf_ln = stats.lognorm.pdf(x_exp, s=sigma, scale=np.exp(mu))
    plt.plot(x_exp, pdf_ln, 'g:', label=f"Current Lognormal (s={sigma})")
    
    plt.legend()
    plt.title("Rescaled Inter-Arrival Times Distribution")
    plt.savefig("report_visualizations/arrival_kde_fit.png")
    print("Saved plot to report_visualizations/arrival_kde_fit.png")

if __name__ == "__main__":
    import scipy.stats as stats # Import here for plotting
    fit_arrival_kde()
